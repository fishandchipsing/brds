{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## supress warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import wave\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import feather\n",
    "import librosa\n",
    "import subprocess\n",
    "import librosa.display\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=5)\n",
    "from matplotlib.mlab import find\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from bisect import bisect_left\n",
    "from operator import itemgetter\n",
    "from itertools import *\n",
    "\n",
    "def takeClosest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "       return after\n",
    "    else:\n",
    "       return before\n",
    "\n",
    "def parabolic(f, x):\n",
    "    xv = 1/2. * (f[x-1] - f[x+1]) / (f[x-1] - 2 * f[x] + f[x+1]) + x\n",
    "    yv = f[x] - 1/4. * (f[x-1] - f[x+1]) * (xv - x)\n",
    "    return (xv, yv)\n",
    "\n",
    "def freq_from_autocorr(audio_signal, sr):\n",
    "    \"\"\"Estimate frequency using autocorrelation.\"\"\"\n",
    "    # Calculate autocorrelation (same thing as convolution, but with one input\n",
    "    # reversed in time), and throw away the negative lags\n",
    "    audio_signal -= np.mean(audio_signal)  # Remove DC offset\n",
    "    corr = signal.fftconvolve(audio_signal, audio_signal[::-1], mode='full')\n",
    "    corr = corr[len(corr)/2:]\n",
    "    # Find the first low point\n",
    "    d = np.diff(corr)\n",
    "    try:\n",
    "        start = find(d > 0)[0]\n",
    "        # Find the next peak after the low point (other than 0 lag).  This bit is\n",
    "        # not reliable for long signals, due to the desired peak occurring between\n",
    "        # samples, and other peaks appearing higher.\n",
    "        i_peak = np.argmax(corr[start:]) + start\n",
    "        i_interp = parabolic(corr, i_peak)[0]\n",
    "        freq = sr / i_interp\n",
    "    except IndexError as e:\n",
    "        # index could not be found, set the pitch to frequency 0\n",
    "        freq = float('nan')\n",
    "    \n",
    "    # The voiced speech of a typical adult male will have a fundamental frequency \n",
    "    # from 85 to 180 Hz, and that of a typical adult female from 165 to 255 Hz.\n",
    "    # Lowest Bass E2 (82.41Hz) to Soprano to C6 (1046.50Hz)\n",
    "    \n",
    "    # This number can be set with insight from the full dataset\n",
    "    # if freq < 80 or freq > 1000:\n",
    "    if freq < 100 or freq > 400:\n",
    "        freq = float('nan')\n",
    "        \n",
    "    return freq\n",
    "\n",
    "def load(filename, sr=8000):\n",
    "    \"\"\"\n",
    "    Load a wave file and return the signal, sample rate and number of channels.\n",
    "    Can be any format that libsndfile supports, like .wav, .flac, etc.\n",
    "    \"\"\"\n",
    "    signal, sample_rate = librosa.load(filename, sr=sr)\n",
    "    channels = 1\n",
    "    return signal, sample_rate, channels\n",
    "\n",
    "def analyse_pair(head, midi_filename, freq_func=freq_from_autocorr, num_windows=10, fs=8000, frame_size=256):\n",
    "    \"\"\"Analyse a single input example from the MIR-QBSH dataset.\"\"\"\n",
    "    # set parameters \n",
    "    # sampling rate of the data samples is 8kHz or 8000Hz \n",
    "    # ground truth frame size is at 256\n",
    "    # can oversample for increased resolution\n",
    "\n",
    "    ## Load data and label\n",
    "    fileroot = head + '/' + midi_filename\n",
    "    \n",
    "    # Load the midi data as well\n",
    "    midi_file = midiroot + midi_filename + '.mid'\n",
    "    # print(midiroot)\n",
    "#     midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    midi_data = None \n",
    "    # print(midi_data)\n",
    "    # think about alignment to midi_data.instruments[0].notes\n",
    "    # currently alignment to true midi is NOT handled\n",
    "\n",
    "    # load data\n",
    "    audio_signal, _, _ = load(fileroot+'.wav', fs)\n",
    "    # print('audio_signal', audio_signal)\n",
    "    # print('length of audio_signal', len(audio_signal))\n",
    "\n",
    "    # Load matching true labelled values\n",
    "    # The .pv file contains manually labelled pitch file, \n",
    "    # with frame size = 256 and overlap = 0. \n",
    "    with open(fileroot+'.pv', 'r') as f:\n",
    "        y = []\n",
    "        for line in f:\n",
    "            if float(line) > 0:\n",
    "                y.append(float(line))\n",
    "            else:\n",
    "                y.append(float('nan'))\n",
    "    # length of the true pitch values should match \n",
    "    # the number of audio frames to analyse\n",
    "    # print('length of true pitch values', len(y))\n",
    "\n",
    "    size_match = len(y) == len(audio_signal)/frame_size\n",
    "    # print('size match', size_match)\n",
    "\n",
    "    num_frames = len(audio_signal) / frame_size\n",
    "    # print('num_frames', num_frames)\n",
    "\n",
    "    # extract pitches (candidates), y_hat\n",
    "\n",
    "    # allow for window overlap to handle clean transitions\n",
    "    # solve for num_frames * 4 and then average out for clean sample\n",
    "    window_size = frame_size / num_windows\n",
    "    y_hat_freq = []\n",
    "    for n in range(num_frames):\n",
    "        window_freq = []\n",
    "        for i in range(num_windows):\n",
    "            window_start = 0+n*frame_size+i*window_size\n",
    "            window_end = frame_size+n*frame_size+i*window_size\n",
    "            window_s = audio_signal[window_start:window_end]\n",
    "\n",
    "            # this is where the magic happens\n",
    "            # define the function to extract the frequency from the windowed signal\n",
    "            window_freq.append(freq_func(window_s, fs))\n",
    "        \n",
    "        # append the median of the window frequencies, \n",
    "        # somewhat robust to anomalies\n",
    "        y_hat_freq.append(np.median(window_freq))\n",
    "\n",
    "    # TODO(kory): downsample to the same length as the ground truth\n",
    "    \n",
    "    # One approach is to remove the outlier points by eliminating any \n",
    "    # points that were above (Mean + 2*SD) and any points below (Mean - 2*SD) \n",
    "    # before plotting the frequencies. This can not happen on the frequency itself\n",
    "    # and rather happens on the frequency change, thus smoothing out huge leaps\n",
    "    freq_change = np.abs(np.diff(y_hat_freq))\n",
    "    \n",
    "    # remove any points where the frequency change is drastic\n",
    "    freq_mean = np.nanmean(freq_change)\n",
    "    freq_std = np.nanstd(freq_change)\n",
    "    freq_change_max = freq_mean + freq_std\n",
    "    bad_idx = np.argwhere(freq_change > freq_change_max).flatten()\n",
    "    for i in bad_idx:\n",
    "        y_hat_freq[i] = float('nan')\n",
    "    \n",
    "    # Convert the frequencies to midi notes\n",
    "    y_hat = librosa.hz_to_midi(y_hat_freq)\n",
    "\n",
    "    # print('y_hat', y_hat)\n",
    "    # print('length of estimated pitch values', len(y_hat))\n",
    "\n",
    "    # compare pitches with actual labels, y\n",
    "    squared_error = (y-y_hat)**2\n",
    "    absolute_error = abs(y-y_hat)\n",
    "    mse = np.nanmean(squared_error)\n",
    "    mae = np.nanmean(absolute_error)\n",
    "    # print('MSE', mse)\n",
    "    \n",
    "    # create a version of the frequency distribution with no nans\n",
    "    y_hat_freq_no_nan = [value for value in y_hat_freq if not math.isnan(value)]\n",
    "    \n",
    "    # clean up the pitches\n",
    "    clean_y_hat = cleaned_midi_pitches(y_hat)    \n",
    "    return audio_signal, midi_data, y, y_hat, clean_y_hat, y_hat_freq, y_hat_freq_no_nan, squared_error, mse, absolute_error, mae\n",
    "\n",
    "def save_extracted_pitches(clean_y_hat, fname, reconroot, fs=8000, frame_size=256):\n",
    "    # TODO(korymath): save the extracted pitches and output midi\n",
    "    \n",
    "    # Save the extraced pitches \n",
    "    np.savetxt(fname + '.out', clean_y_hat, delimiter=',')\n",
    "    \n",
    "    # Save the midi rendition\n",
    "    # Create a PrettyMIDI object\n",
    "    song_recon = pretty_midi.PrettyMIDI()\n",
    "    \n",
    "    # Create an Instrument instance\n",
    "    instrument_name = 'Voice Oohs' # 'Voice Oohs' 'Glockenspiel', 'Acoustic Grand Piano'\n",
    "    instrument_program = pretty_midi.instrument_name_to_program(instrument_name)\n",
    "    instrument = pretty_midi.Instrument(program=instrument_program)\n",
    "        \n",
    "    # get the real indecies\n",
    "    real_idx = np.argwhere(~np.isnan(clean_y_hat))\n",
    "\n",
    "    # find groups of real valued signals\n",
    "    consecutive_real = []\n",
    "    for k, g in groupby(enumerate(real_idx.flatten()), lambda (i,x):i-x):\n",
    "        consecutive_real.append(map(itemgetter(1), g))\n",
    "\n",
    "    # initialize the note holding arrays\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    pitches = []\n",
    "    velocities = []\n",
    "    \n",
    "    time_per_step = float(frame_size / (fs * 1.0)) # 0.032 \n",
    "\n",
    "    for group in consecutive_real:\n",
    "        # only build note if it exists for longer than a single step\n",
    "        if len(group) > 1:\n",
    "            start = group[0] * time_per_step\n",
    "            start_times.append(start)\n",
    "            end_times.append(start + len(group) * time_per_step)\n",
    "            pitches.append(clean_y_hat[group[0]])\n",
    "            # can pull a dynamic velocity from the amplitude of the wave\n",
    "            velocities.append(110)\n",
    "\n",
    "    for i in range(len(pitches)):\n",
    "        note = pretty_midi.Note(velocity=velocities[i], pitch=int(pitches[i]), \n",
    "                                start=start_times[i], \n",
    "                                end=end_times[i])\n",
    "        # print(note)\n",
    "        # append the note to the instrument\n",
    "        instrument.notes.append(note)\n",
    "            \n",
    "    # Add the instrument to the PrettyMIDI object\n",
    "    song_recon.instruments.append(instrument)\n",
    "    \n",
    "    # Write out the MIDI data\n",
    "    revised_file_name = '-'.join(fname.split('/')[3:])[:-4] + '-' + instrument_name.replace(\" \", \"_\") + '.recon.mid'\n",
    "    song_recon.write(reconroot + revised_file_name)\n",
    "    \n",
    "    midi_f = reconroot + revised_file_name\n",
    "    wav_f = reconroot + revised_file_name[:-4] + '.wav'\n",
    "    subprocess.call(['timidity', midi_f, '-Ow', '-o', wav_f])\n",
    "    \n",
    "    # TODO(korymath): attempt to lock to the most prominent key\n",
    "    # Compute the relative amount of each semitone across the entire song, a proxy for key\n",
    "    # total_velocity = sum(sum(cello_c_chord.get_chroma()))\n",
    "    # relative_semitones = [sum(semitone)/total_velocity for semitone in cello_c_chord.get_chroma()]\n",
    "\n",
    "    # keys = [\"C\",\"C#/Db\",\"D\",\"D#/Eb\",\"E\",\"F\",\"F#/Gb\",\"G\",\"G#/Ab\",\"A\",\"A#/Bb\",\"B\"]\n",
    "    # # print('key:', keys[np.argmax(relative_semitones)])\n",
    "\n",
    "    # notes = {}\n",
    "    # notes['C'] = [60,62,64,65,67,69,71,72]\n",
    "\n",
    "    # for note in cello_c_chord.instruments[0].notes:\n",
    "    #     note.pitch = takeClosest(notes['C'], note.pitch)\n",
    "\n",
    "    # # Write out the MIDI data\n",
    "    # cello_c_chord.write(fname + '.cello-key-of-C.mid')\n",
    "\n",
    "    return song_recon\n",
    "\n",
    "def cleaned_midi_pitches(old_y_hat):\n",
    "    # find NaN gaps of 1 and infill\n",
    "    # find larger NaN gaps and create segments\n",
    "    # use median in segment to define note\n",
    "    # use length of segment to define note duration\n",
    "\n",
    "    # initialize our recovery midi pitches\n",
    "    y_hat_clean = np.copy(old_y_hat)\n",
    "\n",
    "    # get indecies of nan values\n",
    "    nan_idx = np.argwhere(np.isnan(y_hat_clean))\n",
    "\n",
    "    # interpolate over single nans \n",
    "    consecutive_nans = []\n",
    "    for k, g in groupby(enumerate(nan_idx.flatten()), lambda (i,x):i-x):\n",
    "        consecutive_nans.append(map(itemgetter(1), g))\n",
    "\n",
    "    for group in consecutive_nans:\n",
    "        if len(group) == 1:\n",
    "            # print('single nan at ', group[0])\n",
    "            y_hat_clean[group[0]] = ((y_hat_clean[group[0]+1])+(y_hat_clean[group[0]-1]))/2\n",
    "            # print(group[0], y_hat_clean[group[0]])\n",
    "\n",
    "    # get idx of real valued signals\n",
    "    real_idx = np.argwhere(~np.isnan(y_hat_clean))\n",
    "\n",
    "    # find groups of real valued signals\n",
    "    consecutive_real = []\n",
    "    for k, g in groupby(enumerate(real_idx.flatten()), lambda (i,x):i-x):\n",
    "        consecutive_real.append(map(itemgetter(1), g))\n",
    "    # for each group, the median is a good estimate of the midi pitch class\n",
    "    for group in consecutive_real:\n",
    "        # TODO(korymath): could be the mean? \n",
    "        # find the median of the real note group\n",
    "        y_hat_clean[group] = np.median(y_hat_clean[group])\n",
    "    return y_hat_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of subjects:', 195)\n",
      "('Subject:', 'Dataset/MIR-QBSH-corpus/waveFile/year2003/person00001/')\n",
      "('Mean MAE so far', 0.16858915098763241)\n",
      "Done processing samples\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the data in to training samples X and labels y\n",
    "# 2. Split the data into training, testing and validation\n",
    "# 3. Test the naive model (autocorrelation) on the test set, this is the baseline to beat\n",
    "# 3a. Evaluation metric is pitch value MSE compared to groundtruth\n",
    "# 4. Train a new model, compare with naive model\n",
    "\n",
    "# Data: Roger Jang's MIR-QBSH corpus which is comprised of 8kHz\n",
    "# 4431 queries along with 48 ground-truth MIDI files. \n",
    "# All queries are from the beginning of references. \n",
    "# Manually labeled pitch for each recording is available. \n",
    "# hand labels are more important to match than ground truth\n",
    "# multiclass classification example\n",
    "\n",
    "# build the filenames incrementally\n",
    "# may need to change based on the location of the datasets\n",
    "reconroot = 'Dataset/MIR-QBSH-corpus/recon/'\n",
    "wavroot = 'Dataset/MIR-QBSH-corpus/waveFile/'\n",
    "midiroot = 'Dataset/MIR-QBSH-corpus/midiFile/'\n",
    "\n",
    "# build a list of all the subjects\n",
    "subjects = []\n",
    "for dirpath, dirnames, filenames in os.walk(wavroot):\n",
    "    if not dirnames:\n",
    "        subjects.append(str(dirpath) + '/')\n",
    "\n",
    "print('number of subjects:', len(subjects))\n",
    "        \n",
    "# build a dictionary to collect the errors\n",
    "errors = {}\n",
    "\n",
    "# build an array to visualize frequency distribtion over all samples\n",
    "y_hat_freq_no_nan_all = []\n",
    "\n",
    "# set the function to extract the fundamental frequency\n",
    "freq_func = freq_from_autocorr\n",
    "\n",
    "# for each subject\n",
    "# for subject in tqdm(subjects):\n",
    "for subject in subjects[0:1]:\n",
    "    # get a list of files this subject has recorded\n",
    "    files = glob.glob(subject + \"*.wav\")\n",
    "\n",
    "    # analyse the given audio signal file\n",
    "#     for f in tqdm(files):\n",
    "    for f in files[0:1]:\n",
    "        errors[f] = {}\n",
    "        # split the given file into filename head and tail \n",
    "        head, tail = os.path.split(f)\n",
    "        # midi_filename gives midi_file reference for each nested sample wav/pv pair\n",
    "        midi_filename = tail.split('.')[0]\n",
    "        (audio_signal, midi_data, y, y_hat, clean_y_hat, y_hat_freq, \n",
    "         y_hat_freq_no_nan, squared_error, mse, absolute_error, \n",
    "         mae) = analyse_pair(head, midi_filename, freq_func=freq_func, \n",
    "                             num_windows=4, fs=8000, frame_size=256)\n",
    "        \n",
    "        # save extracted pitches, midi and wav\n",
    "        song_recon = save_extracted_pitches(clean_y_hat, f, reconroot, fs=8000, frame_size=256)\n",
    "        \n",
    "        # extend the frequency list out\n",
    "        y_hat_freq_no_nan_all.extend(y_hat_freq_no_nan)\n",
    "\n",
    "        # Store error metrics in errors dictionary\n",
    "        errors[f]['mse'] = mse\n",
    "        errors[f]['mae'] = mae\n",
    "        errors[f]['filename'] = '/'.join(f.split('/')[-3:])\n",
    "        # print(f, mse)\n",
    "    \n",
    "    # Print the running error mean\n",
    "    error_accumulator = 0\n",
    "    for k,v in errors.iteritems():\n",
    "        error_accumulator += v['mae']\n",
    "    print('Subject:', subject)\n",
    "    print('Mean MAE so far', error_accumulator/len(errors))\n",
    "\n",
    "print('Done processing samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some summary statistics from Pandas\n",
    "df = pd.DataFrame.from_dict(errors, orient='index')\n",
    "# a pitch difference of 1Hz is perceptable to the human ear\n",
    "# http://hyperphysics.phy-astr.gsu.edu/hbase/Sound/earsens.html\n",
    "# thus, want to get the mean absolute error below 1Hz\n",
    "\n",
    "# TODO(korymath): check this fact?\n",
    "# what is the average jump in a midi pitch class?\n",
    "\n",
    "# store the processing of the current experiment\n",
    "current_milli_time = int(round(time.time() * 1000))\n",
    "path = 'analysis/' + str(current_milli_time) + '.feather'\n",
    "feather.write_dataframe(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the errors between the estimated and ground truth \n",
    "# plot the pitches extracted against the ground truth labels\n",
    "# evenly sampled time at 200ms intervals\n",
    "t = np.arange(0., 250., 1)\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(20,20))\n",
    "\n",
    "# plot actual and estimates\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "ax1.plot(t, y, 'r--')\n",
    "ax1.plot(t, y_hat, 'bs')\n",
    "# ax1.plot(t, np.round(y_hat), 'go')\n",
    "ax1.plot(t, clean_y_hat, 'go')\n",
    "ax1.set_title('Actual and Estimated Values')\n",
    "ax1.set(xlabel='Frame', ylabel='MIDI Pitch') \n",
    "ax1.legend([\"actual\", \"estimated\", \"est_clean\"], loc=2, bbox_to_anchor=(1.05, 1), frameon=True)\n",
    "\n",
    "# Plot square errors\n",
    "ax2.plot(absolute_error)\n",
    "ax2.set_title('Absolute Error')\n",
    "ax2.set(xlabel='Frame', ylabel='Absolute Error (log scale)') \n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# tighten up and show\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the estimated frequencies, and the frequency histogram\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, figsize=(20,30))\n",
    "\n",
    "# plot estimated frequencies\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "ax1.plot(t, y_hat_freq, 'r--')\n",
    "ax1.set_title('Estimated frequency')\n",
    "ax1.set(xlabel='Frame', ylabel='Frequency (Hz)')\n",
    "\n",
    "ax2.set_title('Frequency Change')\n",
    "freq_change = np.diff(y_hat_freq)\n",
    "ax2.plot(t[:-1], freq_change)\n",
    "ax1.set(xlabel='Frame', ylabel='Frequency Change (Hz)') \n",
    "\n",
    "# plot frequency histogram\n",
    "ax3.set_title('Frequency distribution')\n",
    "sns.distplot(y_hat_freq_no_nan, ax=ax3)\n",
    "\n",
    "# tighten up and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reading in the saved error dataframe\n",
    "df = feather.read_dataframe(path)\n",
    "\n",
    "# Visualize the full experimental set statistics\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(20,20))\n",
    "\n",
    "# plot estimated frequencies\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "df['mae'].plot(ax=ax1)\n",
    "ax1.set_title('Absolute error for each sample')\n",
    "ax1.set(xlabel='Sample', ylabel='Mean Absolute Error (Hz)') \n",
    "\n",
    "# plot frequency histogram\n",
    "ax2.set_title('Frequency distribution')\n",
    "sns.distplot(y_hat_freq_no_nan_all, ax=ax2)\n",
    "\n",
    "# tighten up and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst 10 transcriptions\n",
    "# Error rate\n",
    "print('Error rate', 100.0 * len(df[df['mae'] > 1]) / len(df[df['mae'] < 1]))\n",
    "df.sort_values(by='mae', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@title Notebook\n",
    "\n",
    "# TODO: alignment to true midi\n",
    "# start to investigate true midi alignment\n",
    "# obviously most of these are going to be poorly aligned\n",
    "\n",
    "# instrument = midi_data.instruments[0]\n",
    "# true_midi_pitches = []\n",
    "# for note in instrument.notes:    \n",
    "#     note_duration = int((note.end - note.start) * 10)\n",
    "#     true_midi_pitches.extend([note.pitch] * note_duration)\n",
    "    \n",
    "# len(true_midi_pitches)\n",
    "\n",
    "# plt.plot(true_midi_pitches)\n",
    "# plt.plot(y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize note trackers for construction\n",
    "#    last_note = 0\n",
    "#     note_number = 0\n",
    "#    note_hold_duration = 0\n",
    "#    starting_time = 0\n",
    "#     for i in range(1, len(clean_y_hat)):        \n",
    "#         # Handle the NaN case\n",
    "#         if y_hat[i] != y_hat[i]:\n",
    "#             note_number = 0\n",
    "#         else:\n",
    "#             # Round to the nearest midi pitch patch\n",
    "#             note_number = np.round(y_hat[i])\n",
    "        \n",
    "#         # set the last note\n",
    "#         if y_hat[i-1] != y_hat[i-1]:\n",
    "#             last_note = 0\n",
    "                                           \n",
    "#         # if this note is the same as the last note, add to the duration\n",
    "#         # frame size is 256, recorded at 8kHz\n",
    "#         # each note in y_hat corresponds to 0.032 seconds\n",
    "#         # this means that MOST samples at ~8s\n",
    "#         if note_number == last_note:\n",
    "#             note_hold_duration += 0.032\n",
    "#             # print(i, 'holding', int(note_number), note_hold_duration)\n",
    "#         else:\n",
    "#             # Create a Note instance for this note\n",
    "#             note = pretty_midi.Note(velocity=100, \n",
    "#                                     pitch=int(last_note), \n",
    "#                                     start=starting_time, \n",
    "#                                     end=starting_time+note_hold_duration)\n",
    "#             # print('current note', note_number, 'last note', last_note)\n",
    "#             # print(i, 'writing', int(last_note), starting_time, starting_time+note_hold_duration)\n",
    "            \n",
    "#             # Add it to our cello instrument\n",
    "#             cello.notes.append(note)\n",
    "#             # set the starting time of the next note\n",
    "#             starting_time += note_hold_duration\n",
    "#             # reset the note_hold_duration\n",
    "#             note_hold_duration = 0\n",
    "#             # update the last note\n",
    "#             last_note = note_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
