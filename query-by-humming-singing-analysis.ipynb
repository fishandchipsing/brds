{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import wave\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import feather\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=5)\n",
    "from matplotlib.mlab import find\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "\n",
    "def parabolic(f, x):\n",
    "    xv = 1/2. * (f[x-1] - f[x+1]) / (f[x-1] - 2 * f[x] + f[x+1]) + x\n",
    "    yv = f[x] - 1/4. * (f[x-1] - f[x+1]) * (xv - x)\n",
    "    return (xv, yv)\n",
    "\n",
    "def freq_from_autocorr(audio_signal, sr):\n",
    "    \"\"\"Estimate frequency using autocorrelation.\"\"\"\n",
    "    # Calculate autocorrelation (same thing as convolution, but with one input\n",
    "    # reversed in time), and throw away the negative lags\n",
    "    audio_signal -= np.mean(audio_signal)  # Remove DC offset\n",
    "    corr = signal.fftconvolve(audio_signal, audio_signal[::-1], mode='full')\n",
    "    corr = corr[len(corr)/2:]\n",
    "    # Find the first low point\n",
    "    d = np.diff(corr)\n",
    "    try:\n",
    "        start = find(d > 0)[0]\n",
    "        # Find the next peak after the low point (other than 0 lag).  This bit is\n",
    "        # not reliable for long signals, due to the desired peak occurring between\n",
    "        # samples, and other peaks appearing higher.\n",
    "        i_peak = np.argmax(corr[start:]) + start\n",
    "        i_interp = parabolic(corr, i_peak)[0]\n",
    "        freq = sr / i_interp\n",
    "    except IndexError as e:\n",
    "        # index could not be found, set the pitch to frequency 0\n",
    "        freq = float('nan')\n",
    "    \n",
    "    # The voiced speech of a typical adult male will have a fundamental frequency \n",
    "    # from 85 to 180 Hz, and that of a typical adult female from 165 to 255 Hz.\n",
    "    # Lowest Bass E2 (82.41Hz) to Soprano to C6 (1046.50Hz)\n",
    "    \n",
    "    # This number can be set with insight from the full dataset\n",
    "    if freq < 100 or freq > 400:\n",
    "        freq = float('nan')\n",
    "        \n",
    "    return freq\n",
    "\n",
    "def load(filename, sr=8000):\n",
    "    \"\"\"\n",
    "    Load a wave file and return the signal, sample rate and number of channels.\n",
    "    Can be any format that libsndfile supports, like .wav, .flac, etc.\n",
    "    \"\"\"\n",
    "    signal, sample_rate = librosa.load(filename, sr=sr)\n",
    "    channels = 1\n",
    "    return signal, sample_rate, channels\n",
    "\n",
    "def analyse_pair(head, midi_filename, num_windows=10, fs=8000, frame_size=256):\n",
    "    \"\"\"Analyse a single input example from the MIR-QBSH dataset.\"\"\"\n",
    "    # set parameters \n",
    "    # sampling rate of the data samples is 8kHz or 8000Hz \n",
    "    # ground truth frame size is at 256\n",
    "    # can oversample for increased resolution\n",
    "\n",
    "    ## Load data and label\n",
    "    fileroot = head + '/' + midi_filename\n",
    "    \n",
    "    # Load the midi data as well\n",
    "    midi_file = midiroot + midi_filename + '.mid'\n",
    "    # print(midiroot)\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    # print(midi_data)\n",
    "    # think about alignment to midi_data.instruments[0].notes\n",
    "    # currently alignment to true midi is NOT handled\n",
    "\n",
    "    # load data\n",
    "    audio_signal, _, _ = load(fileroot+'.wav', fs)\n",
    "    # print('audio_signal', audio_signal)\n",
    "    # print('length of audio_signal', len(audio_signal))\n",
    "\n",
    "    # Load matching true labelled values\n",
    "    # The .pv file contains manually labelled pitch file, \n",
    "    # with frame size = 256 and overlap = 0. \n",
    "    with open(fileroot+'.pv', 'r') as f:\n",
    "        y = []\n",
    "        for line in f:\n",
    "            y.append(float(line))\n",
    "    # length of the true pitch values should match \n",
    "    # the number of audio frames to analyse\n",
    "    # print('length of true pitch values', len(y))\n",
    "\n",
    "    size_match = len(y) == len(audio_signal)/frame_size\n",
    "    # print('size match', size_match)\n",
    "\n",
    "    num_frames = len(audio_signal) / frame_size\n",
    "    # print('num_frames', num_frames)\n",
    "\n",
    "    # extract pitches (candidates), y_hat\n",
    "\n",
    "    # allow for window overlap to handle clean transitions\n",
    "    # solve for num_frames * 4 and then average out for clean sample\n",
    "    num_windows = 10\n",
    "    window_size = frame_size / num_windows\n",
    "    y_hat_freq = []\n",
    "    for n in range(num_frames):\n",
    "        window_freq = []\n",
    "        for i in range(num_windows):\n",
    "            window_start = 0+n*frame_size+i*window_size\n",
    "            window_end = frame_size+n*frame_size+i*window_size\n",
    "            window_s = audio_signal[window_start:window_end]\n",
    "\n",
    "            # this is where the magic happens\n",
    "            # define the function to extract the frequency from the windowed signal\n",
    "            window_freq.append(freq_from_autocorr(window_s, fs))\n",
    "        y_hat_freq.append(np.mean(window_freq))\n",
    "\n",
    "    # downsample to the same length as the ground truth\n",
    "    # Convert the frequencies to midi notes\n",
    "    y_hat = librosa.hz_to_midi(y_hat_freq)\n",
    "\n",
    "    # print('y_hat', y_hat)\n",
    "    # print('length of estimated pitch values', len(y_hat))\n",
    "\n",
    "    # compare pitches with actual labels, y\n",
    "    squared_error = (y-y_hat)**2\n",
    "    absolute_error = abs(y-y_hat)\n",
    "    mse = np.nanmean(squared_error)\n",
    "    mae = np.nanmean(absolute_error)\n",
    "    # print('MSE', mse)\n",
    "    \n",
    "    # create a version of the frequency distribution with no nans\n",
    "    y_hat_freq_no_nan = [value for value in y_hat_freq if not math.isnan(value)]\n",
    "    \n",
    "    return audio_signal, midi_data, y, y_hat, y_hat_freq, y_hat_freq_no_nan, squared_error, mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:00<?, ?it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▍         | 1/21 [00:00<00:15,  1.29it/s]\u001b[A\n",
      " 10%|▉         | 2/21 [00:01<00:14,  1.31it/s]\u001b[A\n",
      " 14%|█▍        | 3/21 [00:02<00:13,  1.32it/s]\u001b[A\n",
      " 19%|█▉        | 4/21 [00:03<00:15,  1.09it/s]\u001b[A\n",
      " 24%|██▍       | 5/21 [00:04<00:16,  1.00s/it]\u001b[A\n",
      " 29%|██▊       | 6/21 [00:05<00:15,  1.04s/it]\u001b[A\n",
      " 33%|███▎      | 7/21 [00:07<00:15,  1.11s/it]\u001b[A\n",
      " 38%|███▊      | 8/21 [00:08<00:15,  1.21s/it]\u001b[A\n",
      " 43%|████▎     | 9/21 [00:09<00:14,  1.24s/it]\u001b[A\n",
      " 48%|████▊     | 10/21 [00:11<00:13,  1.23s/it]\u001b[A\n",
      " 52%|█████▏    | 11/21 [00:12<00:12,  1.22s/it]\u001b[A\n",
      " 57%|█████▋    | 12/21 [00:13<00:10,  1.20s/it]\u001b[A\n",
      " 62%|██████▏   | 13/21 [00:14<00:09,  1.18s/it]\u001b[A\n",
      " 67%|██████▋   | 14/21 [00:15<00:08,  1.17s/it]\u001b[A\n",
      " 71%|███████▏  | 15/21 [00:16<00:06,  1.14s/it]\u001b[A\n",
      " 76%|███████▌  | 16/21 [00:17<00:05,  1.03s/it]\u001b[A\n",
      " 81%|████████  | 17/21 [00:18<00:03,  1.05it/s]\u001b[A\n",
      " 86%|████████▌ | 18/21 [00:19<00:02,  1.12it/s]\u001b[A\n",
      " 90%|█████████ | 19/21 [00:19<00:01,  1.17it/s]\u001b[A\n",
      " 95%|█████████▌| 20/21 [00:20<00:00,  1.22it/s]\u001b[A\n",
      "100%|██████████| 21/21 [00:21<00:00,  1.25it/s]\u001b[A\n",
      "  1%|          | 1/195 [00:21<1:09:05, 21.37s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/23 [00:00<00:16,  1.30it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:01<00:16,  1.31it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:02<00:15,  1.32it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:03<00:14,  1.33it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:03<00:13,  1.32it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:04<00:12,  1.32it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:05<00:12,  1.32it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:06<00:11,  1.32it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:06<00:10,  1.30it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:07<00:09,  1.31it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:08<00:09,  1.30it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:09<00:08,  1.31it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:09<00:07,  1.30it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:10<00:06,  1.30it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:11<00:06,  1.29it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:12<00:05,  1.28it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:13<00:04,  1.28it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:13<00:03,  1.30it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:14<00:03,  1.31it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:15<00:02,  1.31it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:16<00:01,  1.31it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:16<00:00,  1.32it/s]\u001b[A\n",
      "100%|██████████| 23/23 [00:17<00:00,  1.25it/s]\u001b[A\n",
      "  1%|          | 2/195 [00:39<1:05:11, 20.27s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:15,  1.23it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:01<00:14,  1.26it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:02<00:14,  1.18it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:03<00:13,  1.16it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:04<00:12,  1.18it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:05<00:11,  1.19it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:05<00:11,  1.18it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:06<00:10,  1.20it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:07<00:09,  1.20it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:08<00:08,  1.21it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:09<00:07,  1.22it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:09<00:06,  1.23it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:10<00:05,  1.23it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:11<00:04,  1.23it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:12<00:04,  1.21it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:13<00:03,  1.16it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:14<00:02,  1.11it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:15<00:01,  1.08it/s]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:16<00:00,  1.07it/s]\u001b[A\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\u001b[A\n",
      "  2%|▏         | 3/195 [00:56<1:01:51, 19.33s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [00:00<00:15,  1.26it/s]\u001b[A\n",
      " 10%|█         | 2/20 [00:01<00:14,  1.23it/s]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:02<00:13,  1.25it/s]\u001b[A\n",
      " 20%|██        | 4/20 [00:03<00:12,  1.27it/s]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:03<00:11,  1.26it/s]\u001b[A\n",
      " 30%|███       | 6/20 [00:04<00:11,  1.24it/s]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:05<00:10,  1.20it/s]\u001b[A\n",
      " 40%|████      | 8/20 [00:06<00:10,  1.17it/s]\u001b[A\n",
      " 45%|████▌     | 9/20 [00:07<00:09,  1.13it/s]\u001b[A\n",
      " 50%|█████     | 10/20 [00:08<00:08,  1.12it/s]\u001b[A\n",
      " 55%|█████▌    | 11/20 [00:09<00:08,  1.12it/s]\u001b[A\n",
      " 60%|██████    | 12/20 [00:10<00:07,  1.12it/s]\u001b[A\n",
      " 65%|██████▌   | 13/20 [00:11<00:06,  1.10it/s]\u001b[A\n",
      " 70%|███████   | 14/20 [00:12<00:05,  1.06it/s]\u001b[A\n",
      " 75%|███████▌  | 15/20 [00:13<00:04,  1.07it/s]\u001b[A\n",
      " 80%|████████  | 16/20 [00:14<00:03,  1.09it/s]\u001b[A\n",
      " 85%|████████▌ | 17/20 [00:15<00:02,  1.03it/s]\u001b[A\n",
      " 90%|█████████ | 18/20 [00:16<00:02,  1.03s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [00:17<00:01,  1.02s/it]\u001b[A\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.01s/it]\u001b[A\n",
      "  2%|▏         | 4/195 [01:14<1:00:33, 19.02s/it]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]\u001b[A\n",
      "  8%|▊         | 2/24 [00:02<00:24,  1.09s/it]\u001b[A\n",
      " 12%|█▎        | 3/24 [00:03<00:23,  1.10s/it]\u001b[A\n",
      " 17%|█▋        | 4/24 [00:04<00:22,  1.14s/it]\u001b[A\n",
      " 21%|██        | 5/24 [00:05<00:21,  1.16s/it]\u001b[A\n",
      " 25%|██▌       | 6/24 [00:06<00:20,  1.16s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "# 1. Load the data in to training samples X and labels y\n",
    "# 2. Split the data into training, testing and validation\n",
    "# 3. Test the naive model (autocorrelation) on the test set, this is the baseline to beat\n",
    "# 3a. Evaluation metric is pitch value MSE compared to groundtruth\n",
    "# 4. Train a new model, compare with naive model\n",
    "# Data: Roger Jang's MIR-QBSH corpus which is comprised of 8kHz\n",
    "# 4431 queries along with 48 ground-truth MIDI files. \n",
    "# All queries are from the beginning of references. \n",
    "# Manually labeled pitch for each recording is available. \n",
    "# hand labels are more important to match than ground truth\n",
    "# multiclass classification example\n",
    "\n",
    "# build the filenames incrementally\n",
    "wavroot = 'MIR-QBSH-corpus/waveFile/'\n",
    "midiroot = 'MIR-QBSH-corpus/midiFile/'\n",
    "\n",
    "# build a list of all the subjects\n",
    "subjects = []\n",
    "for dirpath, dirnames, filenames in os.walk(wavroot):\n",
    "    if not dirnames:\n",
    "        subjects.append(str(dirpath) + '/')\n",
    "\n",
    "# build a dictionary to collect the errors\n",
    "errors = {}\n",
    "\n",
    "# build an array to visualize frequency distribtion over all samples\n",
    "y_hat_freq_no_nan_all = []\n",
    "\n",
    "# for each subject\n",
    "for subject in tqdm(subjects):\n",
    "    # get a list of files this subject has recorded\n",
    "    files = glob.glob(subject + \"*.wav\")\n",
    "\n",
    "    # analyse the given audio signal file\n",
    "    for f in tqdm(files):\n",
    "        errors[f] = {}\n",
    "        # split the given file into filename head and tail \n",
    "        head, tail = os.path.split(f)\n",
    "        # midi_filename gives midi_file reference for each nested sample wav/pv pair\n",
    "        midi_filename = tail.split('.')[0]\n",
    "        audio_signal, midi_data, y, y_hat, y_hat_freq, y_hat_freq_no_nan, squared_error, mse, mae = analyse_pair(head, midi_filename, \n",
    "                                        num_windows=10, fs=8000, frame_size=256)\n",
    "        \n",
    "        # extend the frequency list out\n",
    "        y_hat_freq_no_nan_all.extend(y_hat_freq_no_nan)\n",
    "\n",
    "        # Store error metrics in errors dictionary\n",
    "        errors[f]['mse'] = mse\n",
    "        errors[f]['mae'] = mae\n",
    "        errors[f]['filename'] = '/'.join(f.split('/')[-3:])\n",
    "        # print(f, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some summary statistics from Pandas\n",
    "df = pd.DataFrame.from_dict(errors, orient='index')\n",
    "df.describe()\n",
    "# a pitch difference of 1Hz is perceptable to the human ear\n",
    "# http://hyperphysics.phy-astr.gsu.edu/hbase/Sound/earsens.html\n",
    "# thus, want to get the mean absolute error below 1Hz\n",
    "\n",
    "# store the processing of the current experiment\n",
    "current_milli_time = int(round(time.time() * 1000))\n",
    "path = 'analysis/' + str(current_milli_time) + '.feather'\n",
    "feather.write_dataframe(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the errors between the estimated and ground truth \n",
    "# plot the pitches extracted against the ground truth labels\n",
    "# evenly sampled time at 200ms intervals\n",
    "t = np.arange(0., 250., 1)\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(20,20))\n",
    "\n",
    "# plot actual and estimates\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "ax1.plot(t, y, 'r--')\n",
    "ax1.plot(t, y_hat, 'bs')\n",
    "ax1.set_title('Actual and Estimated Values')\n",
    "ax1.set(xlabel='Frame', ylabel='MIDI Pitch') \n",
    "ax1.legend([\"actual\", \"estimated\"], loc=2, bbox_to_anchor=(1.05, 1), frameon=True)\n",
    "\n",
    "# Plot square errors\n",
    "ax2.plot(squared_error)\n",
    "ax2.set_title('Squared Error')\n",
    "ax2.set(xlabel='Frame', ylabel='Squared Error (log scale)') \n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# tighten up and show\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the estimated frequencies, and the frequency histogram\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(20,20))\n",
    "\n",
    "# plot estimated frequencies\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "ax1.plot(t, y_hat_freq, 'r--')\n",
    "ax1.set_title('Estimated frequency')\n",
    "ax1.set(xlabel='Frame', ylabel='Frequency (Hz)') \n",
    "ax1.legend([\"estimated\"], loc=2, bbox_to_anchor=(1.05, 1), frameon=True)\n",
    "\n",
    "# plot frequency histogram\n",
    "ax2.set_title('Frequency distribution')\n",
    "sns.distplot(y_hat_freq_no_nan, ax=ax2)\n",
    "\n",
    "# tighten up and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reading in the saved error dataframe\n",
    "df = feather.read_dataframe(path)\n",
    "\n",
    "# Visualize the full experimental set statistics\n",
    "f, (ax1, ax2) = plt.subplots(2, figsize=(20,20))\n",
    "\n",
    "# plot estimated frequencies\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "df['mae'].plot(ax=ax1)\n",
    "ax1.set_title('Absolute error for each sample')\n",
    "ax1.set(xlabel='Sample', ylabel='Absolute Error (Hz)') \n",
    "\n",
    "# plot frequency histogram\n",
    "ax2.set_title('Frequency distribution')\n",
    "sns.distplot(y_hat_freq_no_nan_all, ax=ax2)\n",
    "\n",
    "# tighten up and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
